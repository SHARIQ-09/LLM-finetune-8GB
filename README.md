# LLM-finetune-8GB
Instruction fine tuning on 8GB of GPU

# Instruction Fine-Tuning on 8GB GPU

ðŸš€ This repository demonstrates how to run **instruction fine-tuning** of a Large Language Model (LLM) on a **single 8GB GPU**.  
Most people assume you need 24â€“80GB cards for this task â€” this repo proves otherwise, with smart memory optimization techniques.  

---

## âœ¨ Key Highlights
- âœ… Instruction fine-tuning on **limited VRAM (8GB)**  
- âœ… Avoiding OOM errors with **FP16 mixed precision**  
- âœ… Efficient batching & **gradient accumulation**  
- âœ… Layer freezing & PEFT/LoRA techniques  
- âœ… End-to-end working Jupyter Notebook  

---

## ðŸ“‚ Contents
- `notebook.ipynb` â†’ Full workflow for instruction fine-tuning  
- Example dataset preprocessing  
- Training loop with mixed precision + scheduler  
- Evaluation with perplexity and predictions  

---

## âš¡ Requirements
- Python 3.10+  
- PyTorch with CUDA  
- Hugging Face `transformers`, `datasets`, `accelerate`  
- GPU: **NVIDIA 8GB VRAM (e.g., RTX 3060, 3070 laptop)**  

Install dependencies:
```bash
pip install -r requirements.txt

